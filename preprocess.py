# -*- coding: utf-8 -*-
"""Preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jTQneacfOBTFWkBPMANnxIB2vL0-712X
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk import PorterStemmer
import math
from bs4 import BeautifulSoup
from bs4.element import Comment
import pickle

nltk.download('stopwords')

stopwords=nltk.corpus.stopwords.words('english')
ps = PorterStemmer()

inverted_index = {}
tokens = {}

os.getcwd()

path = "/content/gdrive/MyDrive/CS-582_project"
os.chdir(path)
os.getcwd()

web_pages = os.listdir("RetrievedPages")

web_pages_names = []
for i in web_pages:
  web_pages_names.append(i)

len(web_pages_names)

def tag_visible(element):
    if element.parent.name in ['head', 'style', 'script', 'meta', '[document]']:
        return False
    elif isinstance(element, Comment):                     
        return False
    elif re.match(r"[\s\r\n]+",str(element)):              
        return False
    else:
      return True

def visible_text(read_HTML):
  soup = BeautifulSoup(read_HTML, "lxml")
  text = soup.find_all(text=True)         
  displayed_text = filter(tag_visible, text)   
  return " ".join(word.strip() for word in displayed_text)

def frequency_count(data, doc_name, freq):
    for i in data:
      # if i not in freq:
      #   freq[i] = {}
      #   freq[i][doc_name] = 1
      # elif doc_name not in freq[i].keys():
      #   freq[i][doc_name] = 1
      # else:
      #   freq[i][doc_name] += 1  
      value = freq.setdefault(i, {}).get(doc_name,0)
      freq[i][doc_name] =  value + 1
    return freq

word_freq = {}


for i in web_pages_names:
    HTML_page = open("RetrievedPages/" + i, "r", encoding="utf-8")
    read_HTML = HTML_page.read()
    text = visible_text(read_HTML)                 
    text = text.lower()
    text = re.sub('[^a-z]+', ' ', text)                 
    words = text.split()
    stop_words_rem = [i for i in words if (len(i)>2 and i not in stopwords)]
    word_clean = [ps.stem(i) for i in stop_words_rem if len(ps.stem(i))>2]


    inverted_index = frequency_count(word_clean, i, word_freq)

with open("Pickle_Folder/" + "6000_inverted_index.pickle", "wb") as f:
    pickle.dump(inverted_index, f)

