# -*- coding: utf-8 -*-
"""Crawler_Pages.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J8kDYiWZHykStk05I50rYyH_KBHLBfbH
"""

from google.colab import drive
drive.mount('/content/gdrive')

from google.colab import drive
drive.mount('/content/drive')

import os
import pickle
import requests
from collections import deque
from bs4 import BeautifulSoup

print(os.getcwd())
path = "/content/gdrive/MyDrive/CS-582_project"
os.chdir(path)

# number of pages to crawl
page_crawl_limit = 6000

# to make sure error log file is initially empty
error_file = "error_log.txt"
f = open(error_file, "w+")
f.close()

"""**Domain for the crawler**"""

domain = "uic.edu"

# starting url
init_url = "https://cs.uic.edu"

# folder to store retrieved web pages
pages_directory_path = "./RetrievedPages/"

ignored_exten = [
    ".xlsx",
    ".css",
    ".js",
    ".aspx",
    ".png",
    ".jpg",
    ".jpeg",
    ".gif",
    ".svg",
    ".ico",
    ".mp4",
    ".avi",
    ".tar",
    ".gz",
    ".tgz",
    ".zip",
    ".pdf",
    ".doc",
    ".docx",
    ".ppt",
    ".pptx",
    ".xls"]

#  Breadth First Search web traversal for URL'S
url_queue = deque()
url_queue.append(init_url)

# storing traversed URLs
crawled_urls = []
crawled_urls.append(init_url)

# dict to track pages fetched and stored in folder
crawled_pages = {}
page_num = 1

url_queue

while url_queue:
  try:
    # fetch the first url from the queue
    url = url_queue.popleft()
    # HTML code
    request = requests.get(url)
    # print(request)

    if request.status_code == 200:

      parser = BeautifulSoup(request.text, "lxml")
      # anchor tags
      extracted_tags = parser.find_all("a")
      # print(extracted_tags)

      # print("*********************")

      if len(extracted_tags) != 0:
        crawled_pages[page_num] = url
        file_name = pages_directory_path + str(page_num)

        # create a directory if RetrievedPages directory doesn't exist,
        # if exists no changes made to the directory
        os.makedirs(os.path.dirname(file_name), exist_ok=True)

        # save the retrievd web page from the current url
        with open(file_name, "w", encoding="utf-8") as f:
            f.write(request.text)
        f.close()

        for tags in extracted_tags:
          ref_url = tags.get("href")
  

          if (ref_url is not None and ref_url.startswith("http")):
            if (not any (i in ref_url for i in ignored_exten)):
              ref_url = ref_url.lower()
              ref_url = ref_url.split('#')[0]
              ref_url = ref_url.split('?', maxsplit=1)[0]
              ref_url = ref_url.rstrip('/')
              ref_url = ref_url.strip()

            if ref_url not in crawled_urls and domain in ref_url:
              url_queue.append(ref_url)                 
              crawled_urls.append(ref_url)
            
        if (len(crawled_pages) > page_crawl_limit):
          break     
        page_num += 1        

  except Exception as e:
      with open(error_file, "a+") as f:
                  # add error message to error log
        f.write(f"Unable to connect to {url}")
        f.write(f"\nError occured: {e}\n\n")
      f.close()

      print("Unable to connect to ", url)
      print("Error occured: ", e, " \n")
      continue

pickle_folder = "./Pickle_Folder/"
os.makedirs(pickle_folder, exist_ok=True)

# Pickling the dict of crawled pages
with open(pickle_folder + '6000_url_pages.pickle', 'wb') as f:
    pickle.dump(crawled_pages,f)


with open(pickle_folder + '6000_url_pages.pickle', 'rb') as f:
    url_pages = pickle.load(f)

crawled_pages == url_pages

len(crawled_pages)

